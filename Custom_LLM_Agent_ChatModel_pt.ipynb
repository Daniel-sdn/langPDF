{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom MRKL agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este notebook explica como criar seu próprio agente MRKL personalizado.\n",
    "\n",
    "Um agente MRKL consiste em três partes:\n",
    "\n",
    "- tools (Ferramentas): as ferramentas que o agente tem disponíveis para uso.\n",
    "\n",
    "- LLMChain: O LLMChain que produz o texto que é analisado de uma determinada maneira para determinar qual ação tomar.\n",
    "\n",
    "- A própria classe do agente: analisa a saída do LLMChain para determinar qual ação tomar.\n",
    "\n",
    "Neste notebook, explicamos como criar um agente MRKL personalizado criando um LLMChain personalizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> <mark>Custom LLMChain </mark></h3>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira maneira de criar um agente customizado é usar uma classe Agent existente, mas usar um LLMChain customizado. Esta é a maneira mais simples de criar um Agente personalizado. É altamente recomendável que você trabalhe com o ZeroShotAgent, pois no momento ele é de longe o mais generalizável.\n",
    "\n",
    "A maior parte do trabalho na criação do LLMChain personalizado se resume ao prompt. Como estamos usando uma classe de agente existente para analisar a saída, é muito importante que o prompt diga para produzir texto nesse formato. Além disso, atualmente exigimos uma variável de entrada agent_scratchpad para colocar notas sobre ações e observações anteriores. Esta deve quase sempre ser a parte final do prompt. Porém, além dessas instruções, você pode personalizar o prompt como desejar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdanielsdnasci\u001b[0m (\u001b[33mdsn-team\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.12"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/dani-boy/LangChain/langchain/wandb/run-20231022_182239-kmbw2u7v</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dsn-team/relato-ai/runs/kmbw2u7v' target=\"_blank\">golden-microwave-6</a></strong> to <a href='https://wandb.ai/dsn-team/relato-ai' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dsn-team/relato-ai' target=\"_blank\">https://wandb.ai/dsn-team/relato-ai</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dsn-team/relato-ai/runs/kmbw2u7v' target=\"_blank\">https://wandb.ai/dsn-team/relato-ai/runs/kmbw2u7v</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "#os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "#os.environ[\"WANDB_PROJECT\"] = \"langchain-tracing\"\n",
    "import datetime\n",
    "from wandb.sdk.data_types.trace_tree import Trace\n",
    "import openai \n",
    "import wandb\n",
    "wandb.init(project=\"relato-ai\")\n",
    "\n",
    "# Helpers\n",
    "import os\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import ZeroShotAgent, Tool, AgentExecutor\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "from langchain.tools import BaseTool\n",
    "from math import pi\n",
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CircumferenceTool(BaseTool):\n",
    "    name = \"Calculadora de circunferência\"\n",
    "    description = \"use esta ferramenta quando precisar calcular uma circunferência usando o raio de um círculo\"\n",
    "\n",
    "    def _run(self, radius: Union[int, float]):\n",
    "        return float(radius)*2.0*pi\n",
    "    \n",
    "    def _arun(self, radius: Union[int, float]):\n",
    "        raise NotImplementedError(\"Esta ferramenta não suporta assíncrono\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Pesquisa\",\n",
    "        func=search.run,\n",
    "        description=\"útil para quando você precisa responder perguntas sobre eventos atuais\",\n",
    "    ),\n",
    "    Tool(\n",
    "    name=\"Calculadora de circunferência\",\n",
    "    func=CircumferenceTool,\n",
    "    description=\"use esta ferramenta quando precisar calcular uma circunferência usando o raio de um círculo\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"Responda às seguintes perguntas da melhor maneira possível. Você tem acesso às seguintes ferramentas:\"\"\"\n",
    "suffix = \"\"\"Começar! Lembre-se de falar em portugues e com um tom amigável.\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools, prefix=prefix, suffix=suffix, input_variables=[\"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Responda às seguintes perguntas da melhor maneira possível. Você tem acesso às seguintes ferramentas:\n",
      "\n",
      "Pesquisa: útil para quando você precisa responder perguntas sobre eventos atuais\n",
      "\n",
      "Use the following format:\n",
      "\n",
      "Question: the input question you must answer\n",
      "Thought: you should always think about what to do\n",
      "Action: the action to take, should be one of [Pesquisa]\n",
      "Action Input: the input to the action\n",
      "Observation: the result of the action\n",
      "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
      "Thought: I now know the final answer\n",
      "Final Answer: the final answer to the original input question\n",
      "\n",
      "Começar! Lembre-se de falar em portugues e com um tom amigável.\n",
      "\n",
      "Question: {input}\n",
      "{agent_scratchpad}\n"
     ]
    }
   ],
   "source": [
    "print(prompt.template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = ZeroShotAgent(llm_chain=llm_chain, allowed_tools=tool_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: Devo encontrar uma estimativa confiável para essa informação.\n",
      "Action: Pesquisa\n",
      "Action Input: População do Canadá em 2023\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m['Canadá ganhou 1 milhão de habitantes entre o início de 2022 e 2023 – o que é impressionante quando se considera o tamanho da população do ...']\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Agora eu sei a resposta final.\n",
      "Final Answer: A estimativa é que cerca de 38 milhões de pessoas vivam no Canadá em 2023.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'A estimativa é que cerca de 38 milhões de pessoas vivam no Canadá em 2023.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"LANGCHAIN_WANDB_TRACING\"] = \"true\"\n",
    "\n",
    "agent_executor.run(\"Quantas pessoas vivem no Canadá em 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: devo encontrar uma maneira de calcular a circunferência\n",
      "Action: Pesquisa\n",
      "Action Input: como calcular a circunferência de um círculo\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m[{'link': 'https://brasilescola.uol.com.br/matematica/comprimento-area-circunferencia.htm', 'source': 'UOL', 'question': 'Como calcular o comprimento de uma circunferência?', 'answer': '<p>Para calcular o comprimento C de uma circunferência de raio r, utilizamos a seguinte fórmula:</p> <p><span class=\"math-tex\">\\\\(C = 2πr\\\\)</p>', 'votes': 0}, {'link': 'https://brasilescola.uol.com.br/matematica/area-circulo.htm', 'source': 'UOL', 'question': 'Como calcular a área do círculo?', 'answer': '<p>Para calcular a área de um círculo, é suficiente conhecer a medida de seu raio, que', 'votes': 0}, {'link': 'https://pt.quora.com/Como-aprender-a-calcular-uma-circunfer%C3%AAncia', 'source': 'Quora', 'question': 'Como aprender a calcular uma circunferência?', 'answer': 'Vamos pensar um pouco, sabemos que [math]\\\\pi = \\\\dfrac{c}{d}[/math] onde c é a circunferência e d o diâmetro de um círculo. Para obtermos a circunferência, basta isolarmos ela. [math]\\\\pi = \\\\dfrac{c}{d}\\\\Rightarrow \\\\pi d = c[/math] PRONTO! Obtemos a circunferência. Sabemos que [math]d = 2r[/math], onde r é o raio do círculo, então podemos escrever a equação acima da seguinte forma utilizando a propriedade comutativa da multiplicação: [math]2\\\\pi r = c[/math]', 'votes': 1}, {'link': 'https://brainly.com.br/tarefa/55729468', 'source': 'Brainly', 'question': 'Como calcular o comprimento da circunferência? E a do círculo? Existe alguma relação entre essas duas medidas?', 'answer': 'Resposta: Comprimento da circunferência é calculado pela fórmula C = 2πr. A área do círculo é A = π·r². Uma relação entre comprimento e área não existe, pois o comprimento é dado em unidades de comprimento (cm, m, etc) e a área é dada em unidades de área (cm², m², etc).', 'votes': 0}, {'link': 'https://www.passeidireto.com/pergunta/121472660/qual-e-a-formula-para-calcular-a-circunferencia-de-um-circulo', 'source': 'Passei Direto', 'question': 'Qual é a fórmula para calcular a circunferência de um círculo?', 'answer': 'A fórmula é C = 2πr, onde C é a circunferência, π é uma constante', 'votes': 1}, {'link': 'https://brainly.com.br/tarefa/57510524', 'source': 'Brainly', 'question': 'Qual é a fórmula para calcular o comprimento da circunferência de um círculo?', 'answer': 'Resposta: A fórmula para descobrir a circunferência a partir do raio é C = 2πr . Explicação passo a passo: Está é uma fórmula bem conhecida na matemática, e que faz muita gente se confundir. Por isso, vamos saber o que cada coisa significa: C = é o comprimento da circunferência. π = é uma proporção numérica definida pela relação entre o comprimento de uma circunferência e seu diâmetro . Geralmente é definido como sendo 3,14. r = é o raio . Espero ter ajudado!', 'votes': 1}, {'link': 'https://www.devmedia.com.br/forum/como-calcular-a-circunferencia/567164', 'source': 'DevMedia', 'question': 'Salve! Preciso calcular os valores de uma circunferência: 1) Fórmula da circunferência -> (2 * pi * r); e 2) Fórmula da área -> (pi ao quadrado); Vejam como eu codifiquei: ...', 'answer': 'O Java trata números decimais como double, por padrão. Logo, na atribuição pi = 3.14159; você está tentando colocar um double dentro de um float. Esse é o motivo do erro... Pra resolver isso, apenas coloque um f na frente do decimal, ficando assim: pi = 3.14159f;...', 'votes': 0}, {'link': 'https://www.passeidireto.com/pergunta/127652480/a-um-circulo-tem-um-raio-de-5-cm-calcule-a-circunferencia-desse-circulo', 'source': 'Passei Direto', 'question': 'a) Um círculo tem um raio de 5 cm. Calcule a circunferência desse círculo.', 'answer': 'Para calcular a circunferência de um círculo, utiliza-se a fórmula C = 2πr, onde C representa a circunferência e r é o raio do círculo. No caso, o raio é de 5 cm. Substituindo na fórmula, temos: C = 2π * 5 C = 10π cm Portanto, a circunferência desse círculo é igual a 10π cm.', 'votes': 0}]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m Agora que eu encontrei a fórmula, posso calcular a circunferência\n",
      "Action: Calcular\n",
      "Action Input: C = 2πr, onde r é o raio de 7.81mm\u001b[0m\n",
      "Observation: Calcular is not a valid tool, try one of [Pesquisa].\n",
      "Thought:\u001b[32;1m\u001b[1;3m Vou usar a fórmula para calcular a circunferência\n",
      "Action: Calcular\n",
      "Action Input: C = 2π * 7.81mm\u001b[0m\n",
      "Observation: Calcular is not a valid tool, try one of [Pesquisa].\n",
      "Thought:\u001b[32;1m\u001b[1;3m Agora que eu tenho a fórmula, posso calcular a circunferência\n",
      "Action: Calcular\n",
      "Action Input: C = 2π * 7.81\u001b[0m\n",
      "Observation: Calcular is not a valid tool, try one of [Pesquisa].\n",
      "Thought:\u001b[32;1m\u001b[1;3m Agora que eu tenho a fórmula, posso calcular a circunferência\n",
      "Action: Calcular\n",
      "Action Input: C = 2π * 7.81mm\u001b[0m\n",
      "Observation: Calcular is not a valid tool, try one of [Pesquisa].\n",
      "Thought:\u001b[32;1m\u001b[1;3m Agora que eu tenho a fórmula, posso usá-la para calcular a circunferência\n",
      "Action: Calcular\n",
      "Action Input: C = 2π * 7.81mm\u001b[0m\n",
      "Observation: Calcular is not a valid tool, try one of [Pesquisa].\n",
      "Thought:\u001b[32;1m\u001b[1;3m Agora que eu tenho a fórmula, posso usá-la para calcular a circunferência\n",
      "Action: Pesquisa\n",
      "Action Input: C = 2π * 7.81mm\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Skipping trace saving - unable to safely convert LangChain Run into W&B Trace due to: 'NoneType' object has no attribute 'items'\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Got error from SerpAPI: Google hasn't returned any results for this query.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/dani-boy/LangChain/langchain/Custom_LLM_Agent_ChatModel_pt.ipynb Cell 15\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-22.04/home/dani-boy/LangChain/langchain/Custom_LLM_Agent_ChatModel_pt.ipynb#Y252sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m agent_executor\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mvocê pode calcular a circunferência de um círculo que tem um raio de 7.81mm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:505\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    504\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 505\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m], callbacks\u001b[39m=\u001b[39;49mcallbacks, tags\u001b[39m=\u001b[39;49mtags, metadata\u001b[39m=\u001b[39;49mmetadata)[\n\u001b[1;32m    506\u001b[0m         _output_key\n\u001b[1;32m    507\u001b[0m     ]\n\u001b[1;32m    509\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    510\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs, callbacks\u001b[39m=\u001b[39mcallbacks, tags\u001b[39m=\u001b[39mtags, metadata\u001b[39m=\u001b[39mmetadata)[\n\u001b[1;32m    511\u001b[0m         _output_key\n\u001b[1;32m    512\u001b[0m     ]\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:310\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 310\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    311\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    312\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    313\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    314\u001b[0m )\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/chains/base.py:304\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    297\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    298\u001b[0m     dumpd(\u001b[39mself\u001b[39m),\n\u001b[1;32m    299\u001b[0m     inputs,\n\u001b[1;32m    300\u001b[0m     name\u001b[39m=\u001b[39mrun_name,\n\u001b[1;32m    301\u001b[0m )\n\u001b[1;32m    302\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 304\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    305\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    306\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    307\u001b[0m     )\n\u001b[1;32m    308\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    309\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:1146\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[0;32m-> 1146\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[1;32m   1147\u001b[0m         name_to_tool_map,\n\u001b[1;32m   1148\u001b[0m         color_mapping,\n\u001b[1;32m   1149\u001b[0m         inputs,\n\u001b[1;32m   1150\u001b[0m         intermediate_steps,\n\u001b[1;32m   1151\u001b[0m         run_manager\u001b[39m=\u001b[39;49mrun_manager,\n\u001b[1;32m   1152\u001b[0m     )\n\u001b[1;32m   1153\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[1;32m   1154\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(\n\u001b[1;32m   1155\u001b[0m             next_step_output, intermediate_steps, run_manager\u001b[39m=\u001b[39mrun_manager\n\u001b[1;32m   1156\u001b[0m         )\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/agents/agent.py:996\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[0;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)\u001b[0m\n\u001b[1;32m    994\u001b[0m         tool_run_kwargs[\u001b[39m\"\u001b[39m\u001b[39mllm_prefix\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    995\u001b[0m     \u001b[39m# We then call the tool on the tool input to get an observation\u001b[39;00m\n\u001b[0;32m--> 996\u001b[0m     observation \u001b[39m=\u001b[39m tool\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    997\u001b[0m         agent_action\u001b[39m.\u001b[39;49mtool_input,\n\u001b[1;32m    998\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[1;32m    999\u001b[0m         color\u001b[39m=\u001b[39;49mcolor,\n\u001b[1;32m   1000\u001b[0m         callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m   1001\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_run_kwargs,\n\u001b[1;32m   1002\u001b[0m     )\n\u001b[1;32m   1003\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1004\u001b[0m     tool_run_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mtool_run_logging_kwargs()\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/tools/base.py:365\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mException\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    364\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_error(e)\n\u001b[0;32m--> 365\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    366\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     run_manager\u001b[39m.\u001b[39mon_tool_end(\n\u001b[1;32m    368\u001b[0m         \u001b[39mstr\u001b[39m(observation), color\u001b[39m=\u001b[39mcolor, name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    369\u001b[0m     )\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/tools/base.py:337\u001b[0m, in \u001b[0;36mBaseTool.run\u001b[0;34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    335\u001b[0m     tool_args, tool_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_args_and_kwargs(parsed_input)\n\u001b[1;32m    336\u001b[0m     observation \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 337\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(\u001b[39m*\u001b[39;49mtool_args, run_manager\u001b[39m=\u001b[39;49mrun_manager, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtool_kwargs)\n\u001b[1;32m    338\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    339\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run(\u001b[39m*\u001b[39mtool_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mtool_kwargs)\n\u001b[1;32m    340\u001b[0m     )\n\u001b[1;32m    341\u001b[0m \u001b[39mexcept\u001b[39;00m ToolException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    342\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandle_tool_error:\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/tools/base.py:516\u001b[0m, in \u001b[0;36mTool._run\u001b[0;34m(self, run_manager, *args, **kwargs)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc:\n\u001b[1;32m    508\u001b[0m     new_argument_supported \u001b[39m=\u001b[39m signature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mcallbacks\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[1;32m    510\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\n\u001b[1;32m    511\u001b[0m             \u001b[39m*\u001b[39margs,\n\u001b[1;32m    512\u001b[0m             callbacks\u001b[39m=\u001b[39mrun_manager\u001b[39m.\u001b[39mget_child() \u001b[39mif\u001b[39;00m run_manager \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    514\u001b[0m         )\n\u001b[1;32m    515\u001b[0m         \u001b[39mif\u001b[39;00m new_argument_supported\n\u001b[0;32m--> 516\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    517\u001b[0m     )\n\u001b[1;32m    518\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTool does not support sync\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/utilities/serpapi.py:85\u001b[0m, in \u001b[0;36mSerpAPIWrapper.run\u001b[0;34m(self, query, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(\u001b[39mself\u001b[39m, query: \u001b[39mstr\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m     84\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run query through SerpAPI and parse result.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_response(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mresults(query))\n",
      "File \u001b[0;32m~/LangChain/langchain/.venv/lib/python3.10/site-packages/langchain/utilities/serpapi.py:131\u001b[0m, in \u001b[0;36mSerpAPIWrapper._process_response\u001b[0;34m(res)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Process response from SerpAPI.\"\"\"\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39merror\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m--> 131\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot error from SerpAPI: \u001b[39m\u001b[39m{\u001b[39;00mres[\u001b[39m'\u001b[39m\u001b[39merror\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    132\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39manswer_box_list\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m res\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m    133\u001b[0m     res[\u001b[39m\"\u001b[39m\u001b[39manswer_box\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m res[\u001b[39m\"\u001b[39m\u001b[39manswer_box_list\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Got error from SerpAPI: Google hasn't returned any results for this query."
     ]
    }
   ],
   "source": [
    "agent_executor.run(\"você pode calcular a circunferência de um círculo que tem um raio de 7.81mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\"Answer the following questions as best you can. You have access to the following tools:\"\"\"\n",
    "suffix = \"\"\"When answering, you MUST speak in the following language: {language}.\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = ZeroShotAgent.create_prompt(\n",
    "    tools,\n",
    "    prefix=prefix,\n",
    "    suffix=suffix,\n",
    "    input_variables=[\"input\", \"language\", \"agent_scratchpad\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=OpenAI(temperature=0), prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = ZeroShotAgent(llm_chain=llm_chain, tools=tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\n",
    "    input=\"How many people live in canada as of 2023?\", language=\"spanish\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom multi-action agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, BaseMultiActionAgent\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SerpAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_word(query: str) -> str:\n",
    "    print(\"\\nNow I'm doing this!\")\n",
    "    return \"foo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events\",\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"RandomWord\",\n",
    "        func=random_word,\n",
    "        description=\"call this to get a random word.\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Any, Union\n",
    "from langchain.schema import AgentAction, AgentFinish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FakeAgent(BaseMultiActionAgent):\n",
    "    \"\"\"Fake Custom Agent.\"\"\"\n",
    "\n",
    "    @property\n",
    "    def input_keys(self):\n",
    "        return [\"input\"]\n",
    "\n",
    "    def plan(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[List[AgentAction], AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        if len(intermediate_steps) == 0:\n",
    "            return [\n",
    "                AgentAction(tool=\"Search\", tool_input=kwargs[\"input\"], log=\"\"),\n",
    "                AgentAction(tool=\"RandomWord\", tool_input=kwargs[\"input\"], log=\"\"),\n",
    "            ]\n",
    "        else:\n",
    "            return AgentFinish(return_values={\"output\": \"bar\"}, log=\"\")\n",
    "\n",
    "    async def aplan(\n",
    "        self, intermediate_steps: List[Tuple[AgentAction, str]], **kwargs: Any\n",
    "    ) -> Union[List[AgentAction], AgentFinish]:\n",
    "        \"\"\"Given input, decided what to do.\n",
    "\n",
    "        Args:\n",
    "            intermediate_steps: Steps the LLM has taken to date,\n",
    "                along with observations\n",
    "            **kwargs: User inputs.\n",
    "\n",
    "        Returns:\n",
    "            Action specifying what tool to use.\n",
    "        \"\"\"\n",
    "        if len(intermediate_steps) == 0:\n",
    "            return [\n",
    "                AgentAction(tool=\"Search\", tool_input=kwargs[\"input\"], log=\"\"),\n",
    "                AgentAction(tool=\"RandomWord\", tool_input=kwargs[\"input\"], log=\"\"),\n",
    "            ]\n",
    "        else:\n",
    "            return AgentFinish(return_values={\"output\": \"bar\"}, log=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = FakeAgent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(\n",
    "    agent=agent, tools=tools, verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"How many people live in canada as of 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle parsing errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Occasionally the LLM cannot determine what step to take because its outputs are not correctly formatted to be handled by the output parser. In this case, by default the agent errors. But you can easily control this functionality with handle_parsing_errors! Let's explore how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.types import AGENT_TO_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this scenario, the agent will error (because it fails to output an Action string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl = initialize_agent(\n",
    "    tools,\n",
    "    ChatOpenAI(temperature=0),\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl.run(\"Who is Leo DiCaprio's girlfriend? No need to add Action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Error Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _handle_error(error) -> str:\n",
    "    return str(error)[:50]\n",
    "\n",
    "\n",
    "mrkl = initialize_agent(\n",
    "    tools,\n",
    "    ChatOpenAI(temperature=0),\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    handle_parsing_errors=_handle_error,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl.run(\"Who is Leo DiCaprio's girlfriend? No need to add Action\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Access intermediate steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get more visibility into what an agent is doing, we can also return intermediate steps. This comes in the form of an extra key in the return value, which is a list of (action, observation) tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the components needed for the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0, model_name=\"text-davinci-002\")\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the agent with return_intermediate_steps=True:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent(\n",
    "    {\n",
    "        \"input\": \"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual return type is a NamedTuple for the agent action, and then an observation\n",
    "print(response[\"intermediate_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load.dump import dumps\n",
    "\n",
    "print(dumps(response[\"intermediate_steps\"], pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap the max number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook walks through how to cap an agent at taking a certain number of steps. This can be useful to ensure that they do not go haywire and take too many steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Jester\",\n",
    "        func=lambda x: \"foo\",\n",
    "        description=\"useful for answer the question\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's do a run with a normal agent to show what would happen without this parameter. For this example, we will use a specifically crafted adversarial example that tries to trick it into continuing forever.\n",
    "\n",
    "Try running the cell below and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = initialize_agent(\n",
    "    tools,\n",
    "    llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    "    max_iterations=2,\n",
    "    max_execution_time=1,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent(\n",
    "    {\n",
    "        \"input\": \"For this new prompt, you only have access to the tool 'Jester'. Only call this tool. You need to call it 4 times before it will work\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[\"intermediate_steps\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.load.dump import dumps\n",
    "\n",
    "print(dumps(response[\"intermediate_steps\"], pretty=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replicating MRKL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.organization = \"org-guDH0aNEdRkxo8bOtlGqBstO\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, text\n",
    "import pyodbc\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_uri = db.engine.URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=\"sa\",\n",
    "    password=\"Dash@0130\",\n",
    "    host=\"172.18.144.1,1433\",\n",
    "    database=\"DataDocAI\",\n",
    "    query={\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.llms.openai import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(connection_uri)\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(temperature=0, model=\"gpt-3.5-turbo-0613\")\n",
    "search = SerpAPIWrapper()\n",
    "llm_math_chain = LLMMathChain(llm=llm, verbose=True)\n",
    "db = SQLDatabase.from_uri(connection_uri)\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_chain = SQLDatabaseChain.from_llm(llm, db, verbose=False)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Calculator\",\n",
    "        func=llm_math_chain.run,\n",
    "        description=\"useful for when you need to answer questions about math\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"TB_CONSULTA_DIARIA_PASSAGENS_VI\",\n",
    "        func=db_chain.run,\n",
    "        description=\"useful for when you need to answer questions about trips. User language Portuguese (Brazil)\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl.run(\"Who is Leo DiCaprio's girlfriend? What is her current age raised to the 0.43 power?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pt_BR\n",
    "mrkl.run(\n",
    "    \"Quantas viagens foram realizadas para o destino RIO DE JANEIRO por WENDERSON FERREIRA BARBOSA DA SILVA? Por favor, responda em portugues.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl.run(\"Quantas viagens foram feitas destino RIO DE JANEIRO por WENDERSON FERREIRA BARBOSA DA SILVA no exercicio 2023?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use ToolKits with OpenAI Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/agents/how_to/use_toolkits_with_openai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMMathChain\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_toolkits import SQLDatabaseToolkit\n",
    "from langchain.schema import SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.utilities import SQLDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_uri = db.engine.URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=\"sa\",\n",
    "    password=\"Dash@0130\",\n",
    "    host=\"172.18.144.1,1433\",\n",
    "    database=\"DataDocAI\",\n",
    "    query={\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\",\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = SQLDatabase.from_uri(connection_uri)\n",
    "toolkit = SQLDatabaseToolkit(db=db, llm=OpenAI(temperature=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toolkit = SQLDatabaseToolkit(llm=ChatOpenAI(), db=db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom LLM Agent (with a ChatModel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook goes through how to create your own custom agent based on a chat model.\n",
    "\n",
    "An LLM chat agent consists of three parts:\n",
    "\n",
    "    - PromptTemplate: This is the prompt template that can be used to instruct the language model on what to do\n",
    "\n",
    "    - ChatModel: This is the language model that powers the agent\n",
    "\n",
    "    - stop sequence: Instructs the LLM to stop generating as soon as this string is found\n",
    "\n",
    "    OutputParser: This determines how to parse the LLM output into an AgentAction or AgentFinish object\n",
    "\n",
    "The LLM Agent is used in an AgentExecutor. This AgentExecutor can largely be thought of as a loop that:\n",
    "\n",
    "    1.  Passes user input and any previous steps to the Agent (in this case, the LLM Agent)\n",
    "\n",
    "    2.  If the Agent returns an AgentFinish, then return that directly to the user\n",
    "\n",
    "    3.  If the Agent returns an AgentAction, then use that to call a tool and get an Observation\n",
    "\n",
    "    4. Repeat, passing the AgentAction and Observation back to the Agent until an AgentFinish is emitted.\n",
    "    \n",
    "AgentAction is a response that consists of action and action_input. action refers to which tool to use, and action_input refers to the input to that tool. log can also be provided as more context (that can be used for logging, tracing, etc).\n",
    "\n",
    "AgentFinish is a response that contains the final message to be sent back to the user. This should be used to end an agent run.\n",
    "\n",
    "In this notebook we walk through how to create a custom LLM agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Types of MessagePromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://python.langchain.com/docs/modules/agents/how_to/custom_llm_chat_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import openai\n",
    "openai.organization = \"org-guDH0aNEdRkxo8bOtlGqBstO\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool, AgentExecutor, LLMSingleActionAgent, AgentOutputParser\n",
    "from langchain.prompts import BaseChatPromptTemplate\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.chains.llm import LLMChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from typing import List, Union\n",
    "from langchain.schema import AgentAction, AgentFinish, HumanMessage\n",
    "import re\n",
    "from getpass import getpass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERPAPI_API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "SERPAPI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Set up tools </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define which tools the agent can use to answer user queries\n",
    "search = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)\n",
    "tools = [\n",
    "    Tool(\n",
    "        name = \"Search\",\n",
    "        func=search.run,\n",
    "        description=\"útil para quando você precisa responder perguntas sobre eventos atuais\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Prompt template </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Isso instrui o agente sobre o que fazer. Geralmente, o modelo deve incorporar:\n",
    "\n",
    "- ferramentas: quais ferramentas o agente tem acesso e como e quando ligar para elas.\n",
    "\n",
    "- intermediário_steps: são tuplas de pares anteriores (AgentAction, Observation). Geralmente, eles não são passados ​​diretamente para o modelo, mas o modelo de prompt os formata de uma maneira específica.\n",
    "\n",
    "- entrada: entrada genérica do usuário"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the base template\n",
    "template = \"\"\"Complete the objective as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "These were previous tasks you completed:\n",
    "\n",
    "\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "{agent_scratchpad}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a prompt template\n",
    "class CustomPromptTemplate(BaseChatPromptTemplate):\n",
    "    # The template to use\n",
    "    template: str\n",
    "    # The list of tools available\n",
    "    tools: List[Tool]\n",
    "    \n",
    "    def format_messages(self, **kwargs) -> str:\n",
    "        # Get the intermediate steps (AgentAction, Observation tuples)\n",
    "        # Format them in a particular way\n",
    "        intermediate_steps = kwargs.pop(\"intermediate_steps\")\n",
    "        thoughts = \"\"\n",
    "        for action, observation in intermediate_steps:\n",
    "            thoughts += action.log\n",
    "            thoughts += f\"\\nObservation: {observation}\\nThought: \"\n",
    "        # Set the agent_scratchpad variable to that value\n",
    "        kwargs[\"agent_scratchpad\"] = thoughts\n",
    "        # Create a tools variable from the list of tools provided\n",
    "        kwargs[\"tools\"] = \"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools])\n",
    "        # Create a list of tool names for the tools provided\n",
    "        kwargs[\"tool_names\"] = \", \".join([tool.name for tool in self.tools])\n",
    "        formatted = self.template.format(**kwargs)\n",
    "        return [HumanMessage(content=formatted)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = CustomPromptTemplate(\n",
    "    template=template,\n",
    "    tools=tools,\n",
    "    # This omits the `agent_scratchpad`, `tools`, and `tool_names` variables because those are generated dynamically\n",
    "    # This includes the `intermediate_steps` variable because that is needed\n",
    "    input_variables=[\"input\", \"intermediate_steps\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O analisador de saída é responsável por analisar a saída do LLM em AgentAction e AgentFinish. Isso geralmente depende muito do prompt usado.\n",
    "\n",
    "É aqui que você pode alterar a análise para fazer novas tentativas, lidar com espaços em branco, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomOutputParser(AgentOutputParser):\n",
    "    \n",
    "    def parse(self, llm_output: str) -> Union[AgentAction, AgentFinish]:\n",
    "        # Check if agent should finish\n",
    "        if \"Final Answer:\" in llm_output:\n",
    "            return AgentFinish(\n",
    "                # Return values is generally always a dictionary with a single `output` key\n",
    "                # It is not recommended to try anything else at the moment :)\n",
    "                return_values={\"output\": llm_output.split(\"Final Answer:\")[-1].strip()},\n",
    "                log=llm_output,\n",
    "            )\n",
    "        # Parse out the action and action input\n",
    "        regex = r\"Action\\s*\\d*\\s*:(.*?)\\nAction\\s*\\d*\\s*Input\\s*\\d*\\s*:[\\s]*(.*)\"\n",
    "        match = re.search(regex, llm_output, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Could not parse LLM output: `{llm_output}`\")\n",
    "        action = match.group(1).strip()\n",
    "        action_input = match.group(2)\n",
    "        # Return the action and action input\n",
    "        return AgentAction(tool=action, tool_input=action_input.strip(\" \").strip('\"'), log=llm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_parser = CustomOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose the LLM you want to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=os.getenv(\"OPENAI_API_KEY\"), temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the stop sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine everything to set up our agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM chain consisting of the LLM and a prompt\n",
    "llm_chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_names = [tool.name for tool in tools]\n",
    "agent = LLMSingleActionAgent(\n",
    "    llm_chain=llm_chain, \n",
    "    output_parser=output_parser,\n",
    "    stop=[\"\\nObservation:\"], \n",
    "    allowed_tools=tool_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor = AgentExecutor.from_agent_and_tools(agent=agent, tools=tools, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_executor.run(\"Procure a namorada atual de Leaonardo Di Caprio?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handle parsing errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMMathChain\n",
    "from langchain.utilities import SerpAPIWrapper\n",
    "from langchain.utilities import SQLDatabase\n",
    "from langchain_experimental.sql import SQLDatabaseChain\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.types import AGENT_TO_CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = SerpAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Search\",\n",
    "        func=search.run,\n",
    "        description=\"useful for when you need to answer questions about current events. You should ask targeted questions\",\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl = initialize_agent(\n",
    "    tools,\n",
    "    ChatOpenAI(temperature=0),\n",
    "    agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrkl.run(\"Who is Leo DiCaprio's girlfriend? No need to add Action\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqlite3 Chinook.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chinook_Sqlite.sqlite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tables-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

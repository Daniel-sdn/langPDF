{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contectando e pesquisando valores no SQLServer 19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://planetscale.com/blog/using-mysql-with-sql-alchemy-hands-on-examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as db\n",
    "from sqlalchemy import create_engine, text\n",
    "import pyodbc\n",
    "import io\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "\n",
    "connection_uri = db.engine.URL.create(\n",
    "    \"mssql+pyodbc\",\n",
    "    username=\"sa\",\n",
    "    password=\"Dash@0130\",\n",
    "    host=\"172.18.144.1,1433\",\n",
    "    database=\"DataDocAI\",\n",
    "    query={\n",
    "        \"driver\": \"ODBC Driver 17 for SQL Server\",\n",
    "    },\n",
    ")\n",
    "engine = create_engine(connection_uri)\n",
    "connection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 8\n",
    "\n",
    "\n",
    "result = connection.execute(text(f\"Select * from  LAI_LEIS_MUNICIPAIS where LEI_CODIGO = {index}\"))\n",
    "for row in result.mappings():\n",
    "    lei_row = row\n",
    "    lei_codigo = row[\"LEI_CODIGO\"]\n",
    "    lei_descricao =  row[\"LEI_DESCRICAO\"]\n",
    "    lei_data_upload = row[\"LEI_DATA_UPLOAD\"]\n",
    "    lei_guid = row[\"LEI_GUID\"]\n",
    "    lei_nome_arquivo = row[\"LEI_NOME_ARQUIVO\"]\n",
    "    lei_ano = row[\"LEI_ANO\"]\n",
    "    lei_decricao_numero = row[\"LEI_DESCRICAO_NUMERO\"]\n",
    "    lei_url = row[\"URL_DOWNLOAD\"]\n",
    "    lei_data = row[\"LEI_DATA\"]\n",
    "    lei_data_ultima = row[\"data_ultima\"]\n",
    "    lei_arquivo = row[\"LEI_ARQUIVO\"]\n",
    "    file_data = row[\"LEI_ARQUIVO\"]\n",
    "    \n",
    "    print(f'data: {lei_data} | Descricao: {lei_descricao} | url: {lei_url}')\n",
    "    \n",
    "    \n",
    "if file_data:\n",
    "    # Crie um arquivo PDF a partir do valor binário.\n",
    "    with open('data/output.pdf', 'wb') as pdf_file:\n",
    "        pdf_file.write(file_data)\n",
    "\n",
    "    print('Arquivo PDF criado com sucesso!')\n",
    "else:\n",
    "    print('O valor binário é nulo.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tratando o PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Portable Document Format (PDF), padronizado como ISO 32000, é um formato de arquivo desenvolvido pela Adobe em 1992 para apresentar documentos, incluindo formatação de texto e imagens, de maneira independente do software aplicativo, hardware e sistemas operacionais.\n",
    "\n",
    "Isso aborda como carregar documentos PDF no formato de documento que usamos posteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "loader = PyPDFLoader(\"data/output.pdf\")\n",
    "pages = loader.load_and_split()\n",
    "pages[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "faiss_index = FAISS.from_documents(pages, OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = faiss_index.similarity_search(\"como será realizada a fiscalização do contrato?\", k=2)\n",
    "for doc in docs:\n",
    "    print(str(doc.metadata[\"page\"]) + \":\", doc.page_content[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Unstructured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredPDFLoader(\"data/output.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Under the hood, Unstructured creates different \"elements\" for different chunks of text. By default we combine those together, but you can easily keep that separation by specifying mode=\"elements\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using PDFMiner to generate HTML text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be helpful for chunking texts semantically into sections as the output html content can be parsed via BeautifulSoup to get more structured and rich information about font size, page numbers, PDF headers/footers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PDFMinerPDFasHTMLLoader\n",
    "loader = PDFMinerPDFasHTMLLoader(\"data/output.pdf\")\n",
    "data = loader.load()[0]   # entire PDF is loaded as a single Document\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "soup = BeautifulSoup(data.page_content,'html.parser')\n",
    "content = soup.find_all('div')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "cur_fs = None\n",
    "cur_text = ''\n",
    "snippets = []   # first collect all snippets that have the same font size\n",
    "for c in content:\n",
    "    sp = c.find('span')\n",
    "    #print(f'\\nsp: {sp}')\n",
    "    if not sp:\n",
    "        continue\n",
    "    st = sp.get('style')\n",
    "    #print(f'\\nst: {st}')\n",
    "    if not st:\n",
    "        continue\n",
    "    fs = re.findall('font-size:(\\d+)px',st)\n",
    "    #print(f'\\nfs: {fs}')\n",
    "    if not fs:\n",
    "        continue\n",
    "    fs = int(fs[0])\n",
    "    if not cur_fs:\n",
    "        cur_fs = fs\n",
    "        #print(f'\\n     cur_fs: {cur_fs}')\n",
    "    if fs == cur_fs:\n",
    "        cur_text += c.text\n",
    "        #print(f'\\n cur_text: {cur_text}')\n",
    "    else:\n",
    "        snippets.append((cur_text,cur_fs))\n",
    "        cur_fs = fs\n",
    "        cur_text = c.text\n",
    "snippets.append((cur_text,cur_fs))\n",
    "# Nota: A lógica acima é muito direta. Também é possível adicionar mais estratégias, como remover trechos duplicados (como\n",
    "# cabeçalhos/rodapés em um PDF aparecem em várias páginas, portanto, se encontrarmos duplicatas, é seguro assumir que são informações redundantes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Isto e uma lista\n",
    "snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "cur_idx = -1\n",
    "semantic_snippets = []\n",
    "# Suposição: os títulos têm tamanho de fonte maior que o respectivo conteúdo\n",
    "for s in snippets:\n",
    "    # se o tamanho da fonte do snippet atual > título da seção anterior => é um novo título\n",
    "    if not semantic_snippets or s[1] > semantic_snippets[cur_idx].metadata['heading_font']:\n",
    "        metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "        metadata.update(data.metadata)\n",
    "        semantic_snippets.append(Document(page_content='',metadata=metadata))\n",
    "        cur_idx += 1\n",
    "        continue\n",
    "\n",
    "    # se o tamanho da fonte do snippet atual <= conteúdo da seção anterior => o conteúdo pertence à mesma seção (também é possível criar\n",
    "    # uma estrutura em forma de árvore para subseções, se necessário, mas isso pode exigir um pouco mais de reflexão e pode ser específico dos dados)\n",
    "    if not semantic_snippets[cur_idx].metadata['content_font'] or s[1] <= semantic_snippets[cur_idx].metadata['content_font']:\n",
    "        semantic_snippets[cur_idx].page_content += s[0]\n",
    "        semantic_snippets[cur_idx].metadata['content_font'] = max(s[1], semantic_snippets[cur_idx].metadata['content_font'])\n",
    "        continue\n",
    "\n",
    "    # se o tamanho da fonte do snippet atual > o conteúdo da seção anterior, mas menor que o título da seção anterior, faça também uma nova\n",
    "    # seção (por exemplo, o título de um PDF terá o tamanho de fonte mais alto, mas não queremos que ele inclua todas as seções)\n",
    "    metadata={'heading':s[0], 'content_font': 0, 'heading_font': s[1]}\n",
    "    metadata.update(data.metadata)\n",
    "    semantic_snippets.append(Document(page_content='',metadata=metadata))\n",
    "    cur_idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(semantic_snippets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_snippets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_snippets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_snippets[1].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_snippets[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_snippets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = semantic_snippets[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = semantic_snippets[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited = texto.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited = [x for x in text_splited if x.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adiciona uma linha em branco entre cada elemento da lista\n",
    "lista_com_linhas_em_branco = [item for sublist in zip(text_splited, [''] * len(text_splited)) for item in sublist]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converte a lista para uma única string, com cada elemento separado por uma quebra de linha\n",
    "texto_final = '\\n'.join(lista_com_linhas_em_branco)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abre o arquivo no modo de escrita ('w') e salva o texto\n",
    "with open('data/doc_pdf.txt', 'w') as arquivo:\n",
    "    arquivo.write(texto_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto_PDF_P = re.sub('\\s+', ' ', text_P).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depois de carregar os documentos, muitas vezes você desejará transformá-los para melhor atender à sua aplicação. O exemplo mais simples é que você pode querer dividir um documento longo em partes menores que caibam na janela de contexto do seu modelo. LangChain possui vários transformadores de documentos integrados que facilitam a divisão, combinação, filtragem e manipulação de documentos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a long document we can split up.\n",
    "with open('data/state_of_the_union.txt') as f:\n",
    "    state_of_the_union = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    # Set a really small chunk size, just to show.\n",
    "    chunk_size = 100,\n",
    "    chunk_overlap  = 40,\n",
    "    length_function = len,\n",
    "    add_start_index = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_to_split = semantic_snippets[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.create_documents([state_of_the_union])\n",
    "print(texts[0])\n",
    "print(texts[1])\n",
    "print(texts[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_count = 0\n",
    "for page in doc:\n",
    "    blocks = page.get_text(\"dict\", flags=0)[\"blocks\"]\n",
    "    for b in blocks:\n",
    "        for l in b[\"lines\"]:\n",
    "            for s in l[\"spans\"]:\n",
    "                text = s[\"text\"]\n",
    "                if chr(0x0024) in text:  # invalid characters encountered!    0xfffd\n",
    "                    # invoke OCR\n",
    "                    ocr_count += 1\n",
    "                    new_text = get_tessocr(page, s)\n",
    "\n",
    "print(\"-------------------------\")\n",
    "print(\"OCR invocations: %i.\" % ocr_count)\n",
    "print(\n",
    "    \"Pixmap time: %g (avg %g) seconds.\"\n",
    "    % (round(PIX_TIME, 5), round(PIX_TIME / ocr_count, 5))\n",
    ")\n",
    "print(\n",
    "    \"OCR time: %g (avg %g) seconds.\"\n",
    "    % (round(OCR_TIME, 5), round(OCR_TIME / ocr_count, 5))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semelhante em conceito ao MarkdownHeaderTextSplitter, o HTMLHeaderTextSplitter é um chunker \"com reconhecimento de estrutura\" que divide o texto no nível do elemento e adiciona metadados para cada cabeçalho \"relevante\" a qualquer pedaço. Ele pode retornar pedaços elemento por elemento ou combinar elementos com os mesmos metadados, com os objetivos de (a) manter o texto relacionado agrupado (mais ou menos) semanticamente e (b) preservar informações ricas em contexto codificadas em estruturas de documentos. Ele pode ser usado com outros divisores de texto como parte de um pipeline de chunking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import HTMLHeaderTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_string =\"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<body>\n",
    "    <div>\n",
    "        <h1>Foo</h1>\n",
    "        <p>Some intro text about Foo.</p>\n",
    "        <div>\n",
    "            <h2>Bar main section</h2>\n",
    "            <p>Some intro text about Bar.</p>\n",
    "            <h3>Bar subsection 1</h3>\n",
    "            <p>Some text about the first subtopic of Bar.</p>\n",
    "            <h3>Bar subsection 2</h3>\n",
    "            <p>Some text about the second subtopic of Bar.</p>\n",
    "        </div>\n",
    "        <div>\n",
    "            <h2>Baz</h2>\n",
    "            <p>Some text about Baz</p>\n",
    "        </div>\n",
    "        <br>\n",
    "        <p>Some concluding text about Foo</p>\n",
    "    </div>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "headers_to_split_on = [\n",
    "    (\"h1\", \"Header 1\"),\n",
    "    (\"h2\", \"Header 2\"),\n",
    "    (\"h3\", \"Header 3\"),\n",
    "]\n",
    "\n",
    "html_splitter = HTMLHeaderTextSplitter(headers_to_split_on=headers_to_split_on)\n",
    "html_header_splits = html_splitter.split_text(html_string)\n",
    "html_header_splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def texto_extraido(texto):\n",
    "    #0. Tratamento da string\n",
    "    text_splited = texto.split('\\n')\n",
    "    text_splited = [s.replace(\":\", \"\") for s in text_splited]\n",
    "    text_splited = [x for x in text_splited if x.strip()]\n",
    "    text_splited = [s.replace(\";\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "    return text_splited"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splited = [s.replace(\"'\", \"\") for s in text_splited]\n",
    "text_splited = [s.replace(\"',\", \"\").strip() for s in text_splited] #depende da situaçao\n",
    "text_splited = texto.split('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "modern-ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
